{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Build a RAG Agent in Langchain\n",
    "\n",
    "Building a Retrieval-Augmented Generation (RAG) agent in Langchain involves several steps, including setting up the environment, loading and processing documents, creating a vector store, and defining the agent's behavior. Below is a step-by-step guide to building a RAG agent in Langchain."
   ],
   "id": "d40cc1d467e9158b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Setup Environment",
   "id": "1ebe7f16fbe10740"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": "!pip install langchain langchain-text-splitters langchain-community bs4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install -U \"langchain-openai\"",
   "id": "f24a5c222e750484",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T19:06:37.965260Z",
     "start_time": "2026-02-15T19:06:27.692673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# Set your OpenAI API key\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ],
   "id": "642baa9bb15ae27",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T19:06:45.676991Z",
     "start_time": "2026-02-15T19:06:45.197045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ],
   "id": "1aaa62336bcf257",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T19:06:47.922727Z",
     "start_time": "2026-02-15T19:06:47.875415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "# Create an in-memory vector store\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ],
   "id": "16043f3787515924",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Indexing Documents\n",
    "\n",
    "Load and process documents to create a vector store that the RAG agent can query for relevant information during generation.\n",
    "\n",
    "LOAD >> SPLIT >> STORE"
   ],
   "id": "698a5efde6ee7799"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Documents",
   "id": "3842ccfcc072222b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T19:06:50.163031Z",
     "start_time": "2026-02-15T19:06:49.807617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Load documents from a web page\n",
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "assert(len(docs) > 0), \"No documents were loaded from the web page.\"\n",
    "print(f\"Loaded {len(docs)} documents from the web page.\")"
   ],
   "id": "99610440354e0bff",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents from the web page.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Split Documents",
   "id": "8fb0811479d90058"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T19:06:56.123859Z",
     "start_time": "2026-02-15T19:06:56.118858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# Split documents into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "print(f\"Split into {len(split_docs)} chunks.\")"
   ],
   "id": "6b883a0729cdb07d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 63 chunks.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Store in Vector Store",
   "id": "ea0c2c6be4bfa057"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T19:06:59.153461Z",
     "start_time": "2026-02-15T19:06:58.164581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "doc_ids = vector_store.add_documents(split_docs)\n",
    "print(f\"Added {len(split_docs)} chunks to the vector store.\")\n",
    "print(\"Document IDs:\", doc_ids)"
   ],
   "id": "e705b4d0b43d73b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 63 chunks to the vector store.\n",
      "Document IDs: ['4a95ac0b-0231-46fb-97b1-8090028c5eeb', '1e7cf92c-54e7-4406-8264-c3c84d8fe57c', 'eb1d727d-fb57-483a-8a75-9ee31944cc81', '98eac3c4-9df4-496e-bea9-40756f230fb4', 'e5df745b-5252-4549-8fe0-b32548440dab', '91ede074-9e2d-41ed-9d8a-9fc82e8e3a06', 'e7a562e6-b66e-4bcc-83ba-b20f347d2f85', '9a6ff857-e4e9-4d12-957d-f5d118437e21', '6219591c-59a0-4de9-9529-dd2b817ee7da', '261a9f1a-a06a-4f6c-96a0-57488c350958', '94f5e9c3-f8e4-4cbf-babf-c721844bc6ad', 'c4f051d8-3543-4c7d-9382-16c9f8f861a7', '4fd36297-9a58-48c5-8e13-3d9ccd865b11', 'dafb858d-e354-4354-8920-36625e83bd9f', '4ee7908a-95c5-4fbc-8837-58d43b8737fa', '6ebceb89-32b6-4ad0-852f-bffb057e4612', 'a0679b80-bce0-4887-903c-66ebf4338d85', '1454ef98-86ff-46c6-b2ce-17560ff9cc22', '51e44e25-7799-4e70-a7cd-cf9de880a4df', 'e0ca545b-3224-4c96-93e8-2477d43a57dc', '9b2bbe3a-a76a-4488-9314-1f1e6f131690', '9470a7c8-e0d7-41e5-b697-ad55dab3a597', '17cc92e3-0caf-4639-8d41-a41a2ad08b06', 'f3f166ee-6434-4d56-a844-54f15d9ba39e', '0200bf71-7fce-4610-926c-62b6fb760929', 'fc51ab4c-da10-4f23-9733-2c0c564e4b1e', 'bd76f7f7-7d97-449f-b3b4-7bb8a026ac5e', '9a3ec7cc-f903-4517-ab7e-4eefa951a8e9', 'dff32bf5-d1ef-4e4e-b107-ee1ecfad3f3b', '4699fb4b-2781-4fe8-99c8-280d09c03d96', '1df06e3c-bbe8-4772-82d3-c7a9ec49d537', '66771fdf-5cbc-4f7b-b9fd-b8adc6576950', '441e5eb1-64f0-49f9-bafa-5660c12aa6a0', '32c53e1d-00eb-4b6a-8147-99362d2a1164', '2f165c8a-96e2-40e0-83d8-db522dca367d', '326c5c0f-dee2-49b2-b8ff-8da01afd0762', 'b0c8670c-004a-4155-a2cb-d88c86b076dc', 'a0d65db6-adfb-4184-a258-8752227a3405', '2f49bd11-c46f-4f86-8a28-c2d99dcc7a8f', '9f1a53ef-845f-4ad6-be66-5fcc0104ec91', '5cabc0b7-aa21-47cd-a546-cb46afa38b74', '5b62f29f-0dd5-41eb-84a2-35c2d51880b4', '6d176c2c-7b7c-40a6-a6cf-2958153df947', '14ec3cdc-fbfc-43a4-9c0e-fb1189061a68', 'df552b7c-9473-4d15-a0d3-efc5fa4574a5', 'a51fbd39-4970-4fd7-8d95-1036aa90172e', 'c267a186-ef1a-4b1d-8958-dfb83ecf886e', '0e4fe5e0-4848-4e87-aee8-7685af309e97', '97c9a3e4-2524-4a67-9c56-91ec8bccb2a8', 'c3a2a720-d59a-4966-8ccb-d8ce8633d550', 'bb3de187-acbb-4987-9a15-20de6aed610a', '304acc4d-c94b-4e95-a036-faadd8f42966', '06403dfc-eb1e-4ccd-b9d7-1cbba0dc8cda', 'f8daeaa9-93a6-40e6-9fa3-cc4ae3e2f485', 'db26afd8-2d11-4f5d-9181-946df3ce07d8', '2288071a-f0e4-4cde-b7ea-6c1f31d16026', 'c821e25f-f89f-418e-8b34-5bf0358b6b1a', '1536feb5-883d-47fb-adf7-2787e44e86cd', '5135b0b1-3ec6-447e-b6b9-d74447c9a5e0', '35a6a925-a890-4a15-bb00-6e00d55e7530', 'ac5a3e5e-c60b-4c7e-b4d1-e6d28321f3e9', '7edce515-191b-4f3e-9291-b34e721c9e90', '7ba275c0-e022-441d-8dde-057d2615788b']\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# RAG Agents",
   "id": "d1b9459a841ad456"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T19:08:01.631345Z",
     "start_time": "2026-02-15T19:08:01.626614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def search_docs(query: str) -> str:\n",
    "    \"\"\"Search for relevant documents in the vector store.\"\"\"\n",
    "    results = vector_store.similarity_search(query, k=3)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in results\n",
    "    )\n",
    "    return serialized, results"
   ],
   "id": "b817b18e33d7978",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T19:08:04.993589Z",
     "start_time": "2026-02-15T19:08:04.987235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(model=\"gpt-4o\")\n",
    "\n",
    "agent = create_agent(\n",
    "    tools=[search_docs],\n",
    "    model=model,\n",
    ")"
   ],
   "id": "c9194e4045199db9",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test the Agent",
   "id": "a3fc818030b92d9b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T19:08:14.104740Z",
     "start_time": "2026-02-15T19:08:07.114496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"What are the key components of a RAG agent?\"\n",
    "\n",
    "for event in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ],
   "id": "91c4043313c6844b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001B[1m Human Message \u001B[0m=================================\n",
      "\n",
      "What are the key components of a RAG agent?\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "Tool Calls:\n",
      "  search_docs (call_dOpAWpI8o1qRuvVSYYPlGtnN)\n",
      " Call ID: call_dOpAWpI8o1qRuvVSYYPlGtnN\n",
      "  Args:\n",
      "    query: key components of a RAG agent\n",
      "=================================\u001B[1m Tool Message \u001B[0m=================================\n",
      "Name: search_docs\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 24126}\n",
      "Content: They also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\n",
      "Generative Agents Simulation#\n",
      "Generative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\n",
      "The design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 8}\n",
      "Content: LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\n",
      "\n",
      "Planning\n",
      "\n",
      "Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\n",
      "Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n",
      "\n",
      "\n",
      "Memory\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 25955}\n",
      "Content: Prompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\n",
      "\n",
      "\n",
      "Planning & Reacting: translate the reflections and the environment information into actions\n",
      "\n",
      "Planning is essentially in order to optimize believability at the moment vs in time.\n",
      "Prompt template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1)\n",
      "Relationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\n",
      "Environment information is present in a tree structure.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The generative agent architecture. (Image source: Park et al. 2023)\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "Tool Calls:\n",
      "  search_docs (call_VmWysaL22quTPlyhyKyZN3Qo)\n",
      " Call ID: call_VmWysaL22quTPlyhyKyZN3Qo\n",
      "  Args:\n",
      "    query: RAG agent components\n",
      "=================================\u001B[1m Tool Message \u001B[0m=================================\n",
      "Name: search_docs\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 24126}\n",
      "Content: They also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\n",
      "Generative Agents Simulation#\n",
      "Generative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\n",
      "The design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 22228}\n",
      "Content: Case Studies#\n",
      "Scientific Discovery Agent#\n",
      "ChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\n",
      "\n",
      "The LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\n",
      "It is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 8}\n",
      "Content: LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\n",
      "\n",
      "Planning\n",
      "\n",
      "Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\n",
      "Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n",
      "\n",
      "\n",
      "Memory\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "A RAG (Retrieval-Augmented Generation) agent consists of several key components:\n",
      "\n",
      "1. **Large Language Model (LLM) Core**: The LLM functions as the agent's brain, driving its decision-making processes and enabling it to understand and generate human-like responses.\n",
      "\n",
      "2. **Document Retriever**: This module allows the agent to retrieve relevant documents from a pre-indexed dataset. It enhances the agent's ability to blend large-scale language model capabilities with specific, accurate information from external sources.\n",
      "\n",
      "3. **Planning Capabilities**: The agent is equipped with the ability to break down complex tasks into smaller subgoals, and to refine these steps through self-reflection, learning from past actions to improve future outcomes.\n",
      "\n",
      "4. **Memory System**: This component enables the agent to store, recall, and utilize past observations and experiences, making the agent's responses more informed and contextually relevant.\n",
      "\n",
      "These components work together to allow a RAG agent to efficiently access and integrate both learned knowledge from the LLM and specific information from external documents, enabling a more robust and context-aware response generation.\n"
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
